{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1777 entries, 0 to 1786\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Tweet    1777 non-null   object\n",
      " 1   Suicide  1777 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 41.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Suicide_Data.csv')\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.drop_duplicates()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suicide\n",
       "Not Suicide post           1124\n",
       "Potential Suicide post      653\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = dataset['Suicide'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi sebelum balancing:\n",
      "Suicide\n",
      "Not Suicide post           1124\n",
      "Potential Suicide post      653\n",
      "Name: count, dtype: int64\n",
      "Distribusi setelah balancing:\n",
      "Suicide\n",
      "Not Suicide post           653\n",
      "Potential Suicide post     653\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "not_suicide = dataset[dataset['Suicide'] == 'Not Suicide post']\n",
    "potential_suicide = dataset[dataset['Suicide'] == 'Potential Suicide post ']\n",
    "\n",
    "not_suicide_downsampled = resample(\n",
    "    not_suicide, \n",
    "    replace=False,             \n",
    "    n_samples=653,             \n",
    "    random_state=42            \n",
    ")\n",
    "\n",
    "balanced_dataset = pd.concat([not_suicide_downsampled, potential_suicide])\n",
    "\n",
    "print(\"Distribusi sebelum balancing:\")\n",
    "print(dataset['Suicide'].value_counts())\n",
    "\n",
    "print(\"Distribusi setelah balancing:\")\n",
    "print(balanced_dataset['Suicide'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Geovanka\\Documents\\Kuliaaaa\\Semester 5\\Natural Language Processing (NLP)\\LAB\\Python 10\\environments\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tweetList = balanced_dataset['Tweet'].to_list()\n",
    "labelList = balanced_dataset['Suicide'].to_list()\n",
    "\n",
    "tfidfVectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')\n",
    "tfidfMatrix = tfidfVectorizer.fit_transform(tweetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Suicide post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Potential Suicide post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "0         Not Suicide post\n",
       "1  Potential Suicide post "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_categories = pd.DataFrame(dataset['Suicide'].unique())\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "punctuation_list = string.punctuation\n",
    "stemming = SnowballStemmer('english')\n",
    "wnl  = WordNetLemmatizer()\n",
    "\n",
    "def removeStopwords(wordList):\n",
    "    removed = []\n",
    "    for word in wordList:\n",
    "        if word not in eng_stopwords:\n",
    "            removed.append(word)\n",
    "            \n",
    "    return removed \n",
    "            \n",
    "def removePunctuation(wordList):\n",
    "    removed = []\n",
    "    for word in wordList:\n",
    "        if word not in punctuation_list:\n",
    "            removed.append(word)\n",
    "            \n",
    "    return removed\n",
    "\n",
    "def removeNumber(wordList):\n",
    "    removed = []\n",
    "    for word in wordList:\n",
    "        if word.isalpha(): \n",
    "            removed.append(word)\n",
    "    return removed\n",
    "\n",
    "def stemmingWord(wordList):\n",
    "    removed = []\n",
    "    for word in wordList:\n",
    "        removed.append(stemming.stem(word))\n",
    "        \n",
    "    return removed\n",
    "\n",
    "def getTag(tag):\n",
    "    if tag =='jj':\n",
    "        return 'a'\n",
    "    elif tag in ['vb', 'nn', 'rb']:\n",
    "        return tag[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatizingWord(wordList):\n",
    "    lemmatizing = []\n",
    "    tagging = pos_tag(wordList)\n",
    "    for word, tag in tagging: \n",
    "        label = getTag(tag.lower())\n",
    "        \n",
    "        if label != None:\n",
    "            lemmatizing.append(wnl.lemmatize(word, label))\n",
    "        else:\n",
    "            lemmatizing.append(wnl.lemmatize(word))\n",
    "            \n",
    "    return lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = removeStopwords(words)\n",
    "    words = removePunctuation(words)\n",
    "    words = removeNumber(words)\n",
    "    words = stemmingWord(words)\n",
    "    words = lemmatizingWord(words)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets = [preprocess_text(tweet) if isinstance(tweet, str) else '' for tweet in tweetList]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_tweets)\n",
    "y = labelList\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 91.84%\n",
      "Naive Bayes Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       Not Suicide post       0.90      0.94      0.92       193\n",
      "Potential Suicide post        0.94      0.89      0.92       199\n",
      "\n",
      "               accuracy                           0.92       392\n",
      "              macro avg       0.92      0.92      0.92       392\n",
      "           weighted avg       0.92      0.92      0.92       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### NAIVE BAYES MODEL\n",
    "\n",
    "labeled_list = list(zip(preprocessed_tweets, y))\n",
    "features_sets = []\n",
    "for sentence, label in labeled_list:\n",
    "    features = {}\n",
    "    check_list = preprocess_text(sentence).split()\n",
    "    for word in vectorizer.get_feature_names_out():\n",
    "        features[word] = (word in check_list)\n",
    "    features_sets.append((features, label))\n",
    "\n",
    "random.shuffle(features_sets)\n",
    "train_count = int(len(features_sets) * 0.7)\n",
    "train_dataset = features_sets[:train_count]\n",
    "test_dataset = features_sets[train_count:]\n",
    "\n",
    "naive_bayes_classifier = NaiveBayesClassifier.train(train_dataset)\n",
    "nb_accuracy = accuracy(naive_bayes_classifier, test_dataset)\n",
    "true_labels = [label for _, label in test_dataset]\n",
    "predicted_labels = [naive_bayes_classifier.classify(features) for features, _ in test_dataset]\n",
    "\n",
    "print(f'Naive Bayes Accuracy: {accuracy(naive_bayes_classifier, test_dataset) * 100:.2f}%')\n",
    "\n",
    "trueLabels = [label for _, label in test_dataset]\n",
    "predictedLabels = [naive_bayes_classifier.classify(features) for features, _ in test_dataset]\n",
    "\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(trueLabels, predictedLabels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 92.35%\n",
      "SVM Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       Not Suicide post       0.91      0.95      0.93       208\n",
      "Potential Suicide post        0.94      0.89      0.92       184\n",
      "\n",
      "               accuracy                           0.92       392\n",
      "              macro avg       0.93      0.92      0.92       392\n",
      "           weighted avg       0.92      0.92      0.92       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SVM MODEL\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 93.11%\n",
      "Random Forest Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       Not Suicide post       0.91      0.97      0.94       208\n",
      "Potential Suicide post        0.96      0.89      0.92       184\n",
      "\n",
      "               accuracy                           0.93       392\n",
      "              macro avg       0.94      0.93      0.93       392\n",
      "           weighted avg       0.93      0.93      0.93       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### RANDOM FOREST MODEL\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 92.09%\n",
      "Logistic Regression Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "       Not Suicide post       0.90      0.95      0.93       208\n",
      "Potential Suicide post        0.94      0.89      0.91       184\n",
      "\n",
      "               accuracy                           0.92       392\n",
      "              macro avg       0.92      0.92      0.92       392\n",
      "           weighted avg       0.92      0.92      0.92       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### LOGISTIC REGRESSION MODEL\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy * 100:.2f}%\")\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 91.33%\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       208\n",
      "           1       0.91      0.90      0.91       184\n",
      "\n",
      "    accuracy                           0.91       392\n",
      "   macro avg       0.91      0.91      0.91       392\n",
      "weighted avg       0.91      0.91      0.91       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "xgboost_model = XGBClassifier(random_state=42)\n",
    "xgboost_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "y_pred_xgb = xgboost_model.predict(X_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%\")\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_xgb, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Random Forest with Accuracy: 93.11%\n"
     ]
    }
   ],
   "source": [
    "### SAVE BEST MODEL\n",
    "\n",
    "results = {\n",
    "    \"Naive Bayes\": nb_accuracy,\n",
    "    \"SVM\": svm_accuracy,\n",
    "    \"Random Forest\": rf_accuracy,\n",
    "    \"Logistic Regression\": lr_accuracy,\n",
    "    \"XGBoost\": xgb_accuracy\n",
    "}\n",
    "\n",
    "best_model_name = max(results, key=results.get)\n",
    "if best_model_name == \"Naive Bayes\":\n",
    "    best_model = naive_bayes_classifier\n",
    "else:\n",
    "    best_model = eval(best_model_name.lower().replace(\" \", \"_\"))\n",
    "\n",
    "with open(\"best_model_rf.pickle\", \"wb\") as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "with open(\"vectorize.pickle\", \"wb\") as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Accuracy: {results[best_model_name] * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
